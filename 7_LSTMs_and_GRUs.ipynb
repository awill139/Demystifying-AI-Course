{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7 - LSTMs and GRUs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awill139/Demystifying-AI-Course/blob/master/7_LSTMs_and_GRUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdAOpFUiydYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKvbJr_eyfvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid( x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "def dsigmoid( x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "    \n",
        "def tangent( x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def dtangent( x):\n",
        "    return 1 - np.tanh(x)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP1myehunbXS",
        "colab_type": "text"
      },
      "source": [
        "#Simple RNN#\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*UkI9za9zTR-HL8uM15Wmzw.png)\n",
        "\n",
        "What is wrong with this?\n",
        "\n",
        "Hard to train\n",
        "\n",
        "Suffer from vanishing gradients, exploding gradients\n",
        "\n",
        "Not easy to work with\n",
        "\n",
        "Hard to remember values from long way in the past\n",
        "\n",
        "We can solve this with a better architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp3NzeeelrJB",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.researchgate.net/profile/S_Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg)\n",
        "\n",
        "Introducing the long short term memory cell or LSTM.\n",
        "\n",
        "LSTM uses what we will call 'gates'\n",
        "\n",
        "There exists the input gate which is just a sidmoid activated input\n",
        "\n",
        "The forget gate which will be the same thing as the input, except we will use it differently, and will scale the hidden state C\n",
        "\n",
        "The output gate which also mimics the input gate\n",
        "\n",
        "The weights of each of these gates will be trained independently\n",
        "\n",
        "\n",
        "We also have the hidden state C. The hidden state is what we remember from the previously given information\n",
        "\n",
        "The big Us in the picture just denote the gate\n",
        "\n",
        "You also may have noticed the g 'gate' but that is nothing more than the original RNN output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxN_2iSzHOeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs=3\n",
        "hidden=5\n",
        "ys=2\n",
        "lr = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfKz74_F9IbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.zeros(xs+ys)\n",
        "xs = xs + ys\n",
        "y = np.zeros(ys)\n",
        "cs = np.zeros(ys)\n",
        "f = np.random.random((ys, xs+ys))\n",
        "i = np.random.random((ys, xs+ys))\n",
        "c = np.random.random((ys, xs+ys))\n",
        "o = np.random.random((ys, xs+ys))\n",
        "Gf = np.zeros_like(f)\n",
        "Gi = np.zeros_like(i)\n",
        "Gc = np.zeros_like(c)\n",
        "Go = np.zeros_like(o)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji8Chwo38vmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forwardProp():\n",
        "    f = sigmoid(np.dot(f, x))\n",
        "    cs *= f\n",
        "    i = sigmoid(np.dot(i, x))\n",
        "    c = tangent(np.dot(c, x))\n",
        "    cs += i * c\n",
        "    o = sigmoid(np.dot(o, x))\n",
        "    y = o * tangent(cs)\n",
        "    return cs, y, f, i, c, o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YifUU-Z888L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backProp( e, pcs, f, i, c, o, dfcs, dfhs):\n",
        "    e = np.clip(e + dfhs, -6, 6)\n",
        "    do = tangent(cs) * e\n",
        "    ou = np.dot(np.atleast_2d(do * dtangent(o)).T, np.atleast_2d(x))\n",
        "    dcs = np.clip(e * o * dtangent(cs) + dfcs, -6, 6)\n",
        "    dc = dcs * i\n",
        "    cu = np.dot(np.atleast_2d(dc * dtangent(c)).T, np.atleast_2d(x))\n",
        "    di = dcs * c\n",
        "    iu = np.dot(np.atleast_2d(di * dsigmoid(i)).T, np.atleast_2d(x))\n",
        "    df = dcs * pcs\n",
        "    fu = np.dot(np.atleast_2d(df * dsigmoid(f)).T, np.atleast_2d(x))\n",
        "    dpcs = dcs * f\n",
        "    dphs = np.dot(dc, c)[:ys] + np.dot(do, o)[:ys] + np.dot(di, i)[:ys] + np.dot(df, f)[:ys] \n",
        "    return fu, iu, cu, ou, dpcs, dphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUf3jgbp8_jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RMSprop\n",
        "def update( fu, iu, cu, ou):\n",
        "    Gf = 0.9 * Gf + 0.1 * fu**2 \n",
        "    Gi = 0.9 * Gi + 0.1 * iu**2   \n",
        "    Gc = 0.9 * Gc + 0.1 * cu**2   \n",
        "    Go = 0.9 * Go + 0.1 * ou**2   \n",
        "    f -= lr/np.sqrt(Gf + 1e-8) * fu\n",
        "    i -= lr/np.sqrt(Gi + 1e-8) * iu\n",
        "    c -= lr/np.sqrt(Gc + 1e-8) * cu\n",
        "    o -= lr/np.sqrt(Go + 1e-8) * ou\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeCtGgSm6EkA",
        "colab_type": "text"
      },
      "source": [
        "Let's try a different architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbQ5Tl4It6Qk",
        "colab_type": "text"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/1600/1*9z1Jrl8K99TorEQfsOTjpA.png)\n",
        "\n",
        "Introducing the Gated Recurrent Unit or GRU\n",
        "\n",
        "Here, we have the update gate in z which will essentially combines the input and forget gate from the LSTM\n",
        "\n",
        "The reset gate in r which behaves a bit like the forget gate in the LSTM\n",
        "\n",
        "The biggest difference is that the GRU lacks an output gate but just uses the hidden state as its output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot4MxPutEVzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialise the weights and biases, and their velocities\n",
        "wstd = 0.2;\n",
        "w1 = np.random.randn(xs,hidden)*wstd\n",
        "w1v = np.zeros((xs,hidden))\n",
        "b1 = np.zeros((hidden,))\n",
        "b1v = np.zeros((hidden,))\n",
        "\n",
        "wz = np.random.randn(2*hidden,hidden)*wstd\n",
        "wzv = np.zeros((2*hidden,hidden)) # the weight velocity\n",
        "bz = np.zeros((hidden,))\n",
        "bzv = np.zeros((hidden,))\n",
        "\n",
        "wr = np.random.randn(2*hidden,hidden)*wstd\n",
        "wrv = np.zeros((2*hidden,hidden)) # the weight velocity\n",
        "br = np.zeros((hidden,))\n",
        "brv = np.zeros((hidden,))\n",
        "\n",
        "wh = np.random.randn(2*hidden,hidden)*wstd\n",
        "whv = np.zeros((2*hidden,hidden)) # the weight velocity\n",
        "bh = np.zeros((hidden,))\n",
        "bhv = np.zeros((hidden,))\n",
        "\n",
        "w2 = np.random.randn(hidden,ys)*wstd\n",
        "w2v = np.zeros((hidden,ys)) # the weight velocity\n",
        "b2 = np.zeros((ys,))\n",
        "b2v = np.zeros((ys,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGuMjzAUEWiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(input):\n",
        "    L = np.shape(input)[0]\n",
        "    az = np.zeros((L,hidden))\n",
        "    ar = np.zeros((L,hidden))\n",
        "    ahhat = np.zeros((L,hidden))\n",
        "    ah = np.zeros((L,hidden))\n",
        "\n",
        "    a1 = tangent(np.dot(input,w1) + b1)\n",
        "    x = np.concatenate((np.zeros((hidden)),a1[1,:]))\n",
        "    az[1,:] = sigmoid(np.dot(x,wz) + bz)\n",
        "    ar[1,:] = sigmoid(np.dot(x,wr) + br)\n",
        "    ahhat[1,:] = tangent(np.dot(x,wh) + bh)\n",
        "    ah[1,:] = az[1,:]*ahhat[1,:]\n",
        "\n",
        "    for i in range(1,L):\n",
        "        x = np.concatenate((ah[i-1,:],a1[i,:]))\n",
        "        az[i,:] = sigmoid(np.dot(x,wz) + bz)\n",
        "        ar[i,:] = sigmoid(np.dot(x,wr) + br)\n",
        "        x = np.concatenate((ar[i,:]*ah[i-1,:],a1[i,:]))\n",
        "        ahhat[i,:] = tangent(np.dot(x,wh) + bh)\n",
        "        ah[i,:] = (1-az[i,:])*ah[i-1,:] + az[i,:]*ahhat[i,:]\n",
        "    a2 = tangent(np.dot(ah,w2) + b2)\n",
        "    return [a1,az,ar,ahhat,ah,a2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS8pYDSHTAxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def compute_gradients(input,labels):\n",
        "        [a1,az,ar,ahhat,ah,a2] = predict(input)\n",
        "        error = (labels - a2)\n",
        "        \n",
        "        L = np.shape(input)[0]\n",
        "        H = hidden\n",
        "        dz = np.zeros((L,H))\n",
        "        dr = np.zeros((L,H))\n",
        "        dh = np.zeros((L,H))\n",
        "        d1 = np.zeros((L,H))\n",
        "\n",
        "        # this is ah from the previous timestep\n",
        "        ahm1 = np.concatenate((np.zeros((1,H)),ah[:-1,:]))\n",
        "\n",
        "        d2 = error*dtangent(a2)\n",
        "        e2 = np.dot(error,w2.T)\n",
        "        dh_next = np.zeros((1,hidden))\n",
        "        for i in range(L-1,-1,-1):\n",
        "            err = e2[i,:] + dh_next\n",
        "            dz[i,:] = (err*ahhat[i,:] - err*ahm1[i,:])*dsigmoid(az[i,:])\n",
        "            dh[i,:] = err*az[i,:]*dtangent(ahhat[i,:])\n",
        "            dr[i,:] = np.dot(dh[i,:],wh[:H,:].T)*ahm1[i,:]*dsigmoid(ar[i,:])\n",
        "            dh_next = err*(1-az[i,:]) + np.dot(dh[i,:],wh[:H,:].T)*ar[i,:] + np.dot(dz[i,:],wz[:H,:].T) + np.dot(dr[i,:],wr[:H,:].T)\n",
        "            d1[i,:] = np.dot(dh[i,:],wh[H:,:].T) + np.dot(dz[i,:],wz[H:,:].T) + np.dot(dr[i,:],wr[H:,:].T)\n",
        "        d1 = d1*dtangent(a1)\n",
        "        # all the deltas are computed, now compute the gradients\n",
        "        gw2 = 1.0/L * np.dot(ah.T,d2)\n",
        "        gb2 = 1.0/L * np.sum(d2,0)\n",
        "        x = np.concatenate((ahm1,a1),1)\n",
        "        gwz = 1.0/L * np.dot(x.T,dz)\n",
        "        gbz = 1.0/L * np.sum(dz,0)\n",
        "        gwr = 1.0/L * np.dot(x.T,dr)\n",
        "        gbr = 1.0/L * np.sum(dr,0)\n",
        "        x = np.concatenate((ar*ahm1,a1),1)\n",
        "        gwh = 1.0/L * np.dot(x.T,dh)\n",
        "        gbh = 1.0/L * np.sum(dh,0)\n",
        "        gw1 = 1.0/L * np.dot(input.T,d1)\n",
        "        gb1 = 1.0/L * np.sum(d1,0)\n",
        "        weight_grads = [gw1,gwr,gwz,gwh,gw2]\n",
        "        bias_grads = [gb1,gbr,gbz,gbh,gb2]\n",
        "        \n",
        "        return weight_grads, bias_grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-JU7cnJTFGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_gradients(input,label,small=0.0001):\n",
        "    weight_grads = []\n",
        "    bias_grads = []\n",
        "    wstr = ['w1','wr','wz','wh','w2']\n",
        "    bstr = ['b1','br','bz','bh','b2']\n",
        "\n",
        "    for i in range(len(wstr)):\n",
        "        w = getattr(wstr[i])\n",
        "        b = getattr(bstr[i])\n",
        "        H,W = np.shape(w)\n",
        "        wgrad = np.zeros((H,W))\n",
        "        bgrad = np.zeros((W,))\n",
        "        for j in range(W):\n",
        "            for k in range(H):\n",
        "                w[k,j] += small\n",
        "                act1 = predict(input)\n",
        "                err1 = np.mean(np.sum(0.5*np.square(label - act1[-1]),1))\n",
        "                w[k,j] -= 2*small\n",
        "                act2 = predict(input)\n",
        "                err2 = np.mean(np.sum(0.5*np.square(label - act2[-1]),1))\n",
        "                wgrad[k,j] = (err1-err2)/(2*small)\n",
        "                w[k,j] += small\n",
        "            b[j] += small\n",
        "            act1 = predict(input)\n",
        "            err1 = np.mean(np.sum(0.5*np.square(label - act1[-1]),1))\n",
        "            b[j] -= 2*small\n",
        "            act2 = predict(input)\n",
        "            err2 = np.mean(np.sum(0.5*np.square(label - act2[-1]),1))\n",
        "            bgrad[j] = (err1-err2)/(2*small)\n",
        "            b[j] += small \n",
        "        weight_grads.append(wgrad)\n",
        "        bias_grads.append(bgrad)\n",
        "    return weight_grads, bias_grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBhMujytTL4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(input,label,momentum=0.9):\n",
        "    weight_grads, bias_grads = compute_gradients(input,labels)\n",
        "    wstr = ['1','r','z','h','2']\n",
        "    for i in range(len(wstr)):\n",
        "        wv = getattr(\"w\"+wstr[i]+\"v\")\n",
        "        wv = momentum*wv + lr*weight_grads[i]\n",
        "        bv = getattr(\"b\"+wstr[i]+\"v\")\n",
        "        bv = momentum*bv + lr*bias_grads[i]\n",
        "        w = getattr(\"w\"+wstr[i])\n",
        "        w += wv\n",
        "        bv = getattr(\"b\"+wstr[i])\n",
        "        b += bv\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVuFOq4F3OVd",
        "colab_type": "text"
      },
      "source": [
        "Which should you choose?\n",
        "Both are fine. LSTMs are more flexible because they have more trainable parameters than GRUs, but this also leads to slower traixsg. Performance is mostly equal, one may do slightly better in a task than another. It is usually good to, once you determined your task, to test it on LSTM, then test it on GRU, compare loss and output and make your decision there and then do further development on this data with the chosen architecture. (Personally I tend to use GRUs for text and LSTMs for images. No real reason, as I've said, the performance is virtually negligible.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjajWhmuKmnr",
        "colab_type": "text"
      },
      "source": [
        "I want to still do something cool, so here is a shakespeare text generation tutorial taken from [Tensorflow Tutorials](https://www.tensorflow.org/beta/tutorials/text/text_generation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2wOcBuyJZNS",
        "colab_type": "code",
        "outputId": "eee71fa7-d21a-44f5-ba49-afa155dfa620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-beta0\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-beta0 in /usr/local/lib/python3.6/dist-packages (2.0.0b0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.33.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.1.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.11.1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.14.0a20190603)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (0.15.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErATX3JgJjk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9hftDA0JlDv",
        "colab_type": "code",
        "outputId": "a5775ed7-23a6-41ef-cc94-7b7368241897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XUgby0aJmvd",
        "colab_type": "code",
        "outputId": "671741cb-2b82-48a8-cf21-9fceaf62e052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiFX9BZQJoWn",
        "colab_type": "code",
        "outputId": "cc4d33cd-a38f-4375-fbfb-10ec6dac0825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_sRJHjgJqUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hw9VffrJrhY",
        "colab_type": "code",
        "outputId": "9418bfbe-5bca-4722-8aff-0bd20b07b0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TZRg_lLJr6Y",
        "colab_type": "code",
        "outputId": "e8929f4b-b8ae-4ec3-b455-1e552afa256c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqGocXNsJugX",
        "colab_type": "code",
        "outputId": "47564625-9795-4250-b470-90cd3fc56431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oDq8suuJv1V",
        "colab_type": "code",
        "outputId": "1e477ad0-3b10-458d-cf4e-62b16f3a564e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4BExXouJxZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBjUXXHpJyru",
        "colab_type": "code",
        "outputId": "ed482a93-8a59-4f27-bc43-82d4884af161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoeErl2pJzy4",
        "colab_type": "code",
        "outputId": "3a0f3a16-e10f-438c-844d-373fa227ffcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waUyCaKBJ1eM",
        "colab_type": "code",
        "outputId": "ea0a8726-1039-4d46-9266-0c421140769b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVM5GwR-Bc7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6c7ibaxJ2eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfahrZ0K9Oc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0E_wGDgJ4nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHaXecyrJ69_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hZ9SkLmJ8Zt",
        "colab_type": "code",
        "outputId": "ff0b3fae-580f-40e0-865c-a1bc8c076482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "186wjmlmJ9dG",
        "colab_type": "code",
        "outputId": "416bfdba-a830-4e80-9130-bf842caf8564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wowvfo-kJ_nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-4sX8PuKBG1",
        "colab_type": "code",
        "outputId": "33f9ce9d-3823-4277-b337-b5cd802f16be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'ble not.\\n\\nBIANCA:\\nBelieve me, sister, of all the men alive\\nI never yet beheld that special face\\nWhic'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"XRcQgFY!s$pY,PftiFjtdvmQs-VvMnYRxfa3RAnulbc;.'NFe'mSw!Pne-:!'!u asYjW G\\n;OEQqmJptF:jeZQn!JZ!X;g;y ;H\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI6UpdKFKCUz",
        "colab_type": "code",
        "outputId": "ae6c262b-85a6-4e60-bd5b-5cd9399467aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1743646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNeW6bF6KDBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x88PTttKEWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKpWtzvMKHrg",
        "colab_type": "code",
        "outputId": "310a5720-1d97-47fa-8bc1-89afe62c4a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "source": [
        "import time\n",
        "EPOCHS=10\n",
        "tic = time.time()\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "toc = time.time()\n",
        "toc - tic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0612 23:22:03.661798 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
            "W0612 23:22:03.663102 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter\n",
            "W0612 23:22:03.663949 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "W0612 23:22:03.666805 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "W0612 23:22:03.669498 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\n",
            "W0612 23:22:03.671381 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "W0612 23:22:03.673059 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "W0612 23:22:03.674807 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "W0612 23:22:03.676509 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "W0612 23:22:03.678399 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "W0612 23:22:03.680021 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "W0612 23:22:03.680970 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "W0612 23:22:03.682560 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "W0612 23:22:03.684234 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "W0612 23:22:03.686264 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "W0612 23:22:03.687942 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "W0612 23:22:03.689961 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "W0612 23:22:03.691951 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "W0612 23:22:03.692822 140499343632256 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 48s 280ms/step - loss: 2.6679\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 47s 274ms/step - loss: 1.9663\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 47s 275ms/step - loss: 1.7016\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 47s 275ms/step - loss: 1.5475\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 47s 274ms/step - loss: 1.4533\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 47s 275ms/step - loss: 1.3879\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 47s 274ms/step - loss: 1.3355\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 47s 273ms/step - loss: 1.2890\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 47s 274ms/step - loss: 1.2455\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 47s 274ms/step - loss: 1.2046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "473.1028571128845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoluAKW1Rr-T",
        "colab_type": "code",
        "outputId": "137750dc-8b4b-4c18-906b-8893e890de54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmYJDLMPRsr9",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing for GRU. Same thing, but there is some size difference somewhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAYBJBrOQoyc",
        "colab_type": "code",
        "outputId": "bfc7c12f-b66f-472a-b136-2eb485b5e550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        }
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])\n",
        "  \n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n",
        "  \n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "  \n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
        "    \n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "def gru_build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "gru_model = gru_build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "gru_model.compile(optimizer = 'adam', loss = loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "gru_checkpoint_dir = './gru_training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "gru_checkpoint_prefix = os.path.join(gru_checkpoint_dir, \"gru_ckpt_{epoch}\")\n",
        "\n",
        "gru_checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=gru_checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n",
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "168hKXk_QcqA",
        "colab_type": "code",
        "outputId": "68b2fa36-93e1-443d-ca23-b57182fc3668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "tic = time.time()\n",
        "GRU_EPOCHS=10\n",
        "gru_history = gru_model.fit(dataset.repeat(), epochs=GRU_EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[gru_checkpoint_callback])\n",
        "toc = time.time()\n",
        "toc - tic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0612 23:29:58.364739 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
            "W0612 23:29:58.365939 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter\n",
            "W0612 23:29:58.368852 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "W0612 23:29:58.371750 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "W0612 23:29:58.374984 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\n",
            "W0612 23:29:58.376468 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "W0612 23:29:58.377783 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "W0612 23:29:58.379308 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "W0612 23:29:58.380789 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "W0612 23:29:58.382472 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "W0612 23:29:58.383758 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "W0612 23:29:58.385344 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "W0612 23:29:58.386940 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "W0612 23:29:58.388692 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "W0612 23:29:58.389948 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "W0612 23:29:58.391642 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "W0612 23:29:58.392890 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "W0612 23:29:58.394554 140499343632256 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "W0612 23:29:58.395800 140499343632256 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "174/174 [==============================] - 47s 273ms/step - loss: 2.6762\n",
            "Epoch 2/10\n",
            "174/174 [==============================] - 46s 263ms/step - loss: 1.9500\n",
            "Epoch 3/10\n",
            "174/174 [==============================] - 46s 264ms/step - loss: 1.6870\n",
            "Epoch 4/10\n",
            "174/174 [==============================] - 46s 264ms/step - loss: 1.5380\n",
            "Epoch 5/10\n",
            "174/174 [==============================] - 46s 264ms/step - loss: 1.4479\n",
            "Epoch 6/10\n",
            "174/174 [==============================] - 46s 264ms/step - loss: 1.3885\n",
            "Epoch 7/10\n",
            "174/174 [==============================] - 46s 264ms/step - loss: 1.3432\n",
            "Epoch 8/10\n",
            "174/174 [==============================] - 46s 263ms/step - loss: 1.3027\n",
            "Epoch 9/10\n",
            "174/174 [==============================] - 46s 263ms/step - loss: 1.2667\n",
            "Epoch 10/10\n",
            "174/174 [==============================] - 46s 264ms/step - loss: 1.2332\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "460.4146156311035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c78M1qhdKPCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4EDGfxZKRQ-",
        "colab_type": "code",
        "outputId": "4042de72-171d-4b07-d4b2-a5b8c4b55857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I'll lay on.\n",
            "\n",
            "POMPEY:\n",
            "See, they do you go?\n",
            "\n",
            "MONTIUS:\n",
            "Tente maid, and not so.\n",
            "\n",
            "LADY ANNE:\n",
            "I would tef the galler. Come home set down by this nightlace of you,\n",
            "While I infect it, I have rear so, but I had rather be\n",
            "murder well again: cannot the reward in the bont's partian.\n",
            "\n",
            "KING RICHARD III:\n",
            "Upon the sicke, or on; but such a worthy day!\n",
            "But do I bear, and learn.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "But that have restrated by him and mine ede?\n",
            "Who shall this trumpet's her. I'll have you will.\n",
            "\n",
            "COMINIUS:\n",
            "That, get thee joy. But she was no parson at many in him,\n",
            "He was it were they are deceived: they are aleck\n",
            "Of this of this sight:\n",
            "Now, charge, God be thought, my son, thou shalt not boye?\n",
            "Weich be with death, and puase.\n",
            "Come, signifa come, we fool it Ifrethe parting of the curs'd with such and two maricards\n",
            "With trial curse, it be well and you not howaft him sout\n",
            "Shake's gentle fall;\n",
            "Then thou wilt would pass him.\n",
            "\n",
            "She's LIZAS:\n",
            "Look'd, morworsul, Gurronanes again together,\n",
            "Was the addived fellow; the gentle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-9SbjovnwBe",
        "colab_type": "code",
        "outputId": "a2447187-a96c-42c0-b353-32ba9cfabf9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "tf.train.latest_checkpoint(gru_checkpoint_dir)\n",
        "\n",
        "gru_model = gru_build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "gru_model.load_weights(tf.train.latest_checkpoint(gru_checkpoint_dir))\n",
        "\n",
        "gru_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS41-hlWMrV7",
        "colab_type": "code",
        "outputId": "22eddfbf-8a85-448b-e8f7-9f16b756196c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "print(generate_text(gru_model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: How was it ere I live, himself, for in the man shall repost\n",
            "As I am angry conquered from his death;\n",
            "It was his Witonly,\n",
            "Will stand going\n",
            "And take the fatal fools:\n",
            "Thistor, fie!\n",
            "follow I fare an your rather makes thanks.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "His grace my son with arry'd mother,' hath been suits stone and cross.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Somes the matter for done so.\n",
            "\n",
            "BRUTUS:\n",
            "And I, but I'll think our titles; and I close it.\n",
            "I will find him to aith I,\n",
            "And with their sufferane shall use upon my son,\n",
            "Thou call'd these three does y,\n",
            "An earth mage sundue that and Edward, ig a swearing powerful?\n",
            "\n",
            "OXFORD:\n",
            "Well, methinks of milen:\n",
            "High, but shall you find accompany;\n",
            "I thank toward tignd his unlook'd my flown\n",
            "Gods thou shalt another faults. I cannot with this prince.\n",
            "\n",
            "LUCENTIO:\n",
            "Sirrah, owe than with thy misdern; and the dege's nurse,\n",
            "An as monest years that brought our aut of Gaunt: but therein some that\n",
            "we better purrish on me: guilty out,\n",
            "That comes his sonand and his cold man would the woes will I,\n",
            "That \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo6Zq3UAMttZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWu1Vo4CMvJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}